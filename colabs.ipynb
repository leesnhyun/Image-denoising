{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ_xD9GH5p6B"
   },
   "source": [
    "#Assignment 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rN60VJww8HVS"
   },
   "source": [
    "Develop a denoising algorithm based on an auto-encoder architecture using pytorch library in the supervised learning framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVmFa0_UndLb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1amYPATWmRY"
   },
   "outputs": [],
   "source": [
    "# custom dataloader for .npy file\n",
    "class numpyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBjyiqXqnj-c"
   },
   "outputs": [],
   "source": [
    "class DenoiseNetwork(nn.Module):\n",
    "    def __init__(self, depth=17, n_channels=64, image_channels=1, kernel_size=3):\n",
    "        super(DenoiseNetwork, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        encoder_layers = []\n",
    "\n",
    "        encoder_layers.append(nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n",
    "        encoder_layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(depth-2):\n",
    "            encoder_layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            encoder_layers.append(nn.BatchNorm2d(n_channels, momentum = 0.95))\n",
    "            encoder_layers.append(nn.ReLU(inplace=True))\n",
    "        encoder_layers.append(nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        encoder_layers.append(nn.Conv2d(1, 1, 1))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(1, 1, 1))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return y-out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.orthogonal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orPzbm3fnmyJ"
   },
   "outputs": [],
   "source": [
    "# import model\n",
    "model = DenoiseNetwork()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.2)\n",
    "objective = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "to_img = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUEI3UXuWmRb"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# for training\n",
    "traindata = np.load('/content/drive/My Drive/Colab Notebooks/train.npy')\n",
    "traindataset = numpyDataset(traindata, transform)\n",
    "trainloader = DataLoader(traindataset, batch_size=4, shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "loss_train_mean, loss_train_std = [], []\n",
    "prev_train_loss, next_train_loss = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZgpPhtinqz5"
   },
   "outputs": [],
   "source": [
    "def train(batch_idx, epoch, clean_image, noisy_image):\n",
    "    clean, noisy = Variable(clean_image).cuda(), Variable(noisy_image).cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(noisy)\n",
    "\n",
    "    loss = objective(output, clean)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    \n",
    "    loss_train_batch = loss.item() / len(data)\n",
    "\n",
    "    return loss_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "skUibzj8WmRd",
    "outputId": "10417929-455f-4513-e981-6f216971af72",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] loss(training): 29.285741989233276\n",
      "[epoch 1] loss(training): 3.433973973230882\n",
      "[epoch 2] loss(training): 3.119905606291511\n",
      "[epoch 3] loss(training): 2.9337451007420365\n",
      "[epoch 4] loss(training): 2.8302755841883744\n",
      "[epoch 5] loss(training): 2.4766171819242566\n",
      "[epoch 6] loss(training): 2.460532487522472\n",
      "[epoch 7] loss(training): 2.3173554904081604\n",
      "[epoch 8] loss(training): 2.265858602740548\n",
      "[epoch 9] loss(training): 2.218878173665567\n",
      "[epoch 10] loss(training): 2.1432215161215176\n",
      "[epoch 11] loss(training): 2.121496638493104\n",
      "[epoch 12] loss(training): 2.1119101673364638\n",
      "[epoch 13] loss(training): 2.020529891333797\n",
      "[epoch 14] loss(training): 2.031985253447836\n",
      "[epoch 15] loss(training): 2.0223069656166164\n",
      "[epoch 16] loss(training): 2.0123845940015532\n",
      "[epoch 17] loss(training): 2.024318671131676\n",
      "[epoch 18] loss(training): 2.029639129449021\n",
      "[epoch 19] loss(training): 1.9201246351545507\n",
      "[epoch 20] loss(training): 1.9809496579657901\n",
      "[epoch 21] loss(training): 1.9148715837028893\n",
      "[epoch 22] loss(training): 1.8610638955912806\n",
      "[epoch 23] loss(training): 1.8792986641282385\n",
      "[epoch 24] loss(training): 1.9038014303554187\n",
      "[epoch 25] loss(training): 1.9143063823878765\n",
      "[epoch 26] loss(training): 1.9174767122891816\n",
      "[epoch 27] loss(training): 1.8500062218985773\n",
      "[epoch 28] loss(training): 1.895456295203079\n",
      "[epoch 29] loss(training): 1.8672964109615846\n",
      "[epoch 30] loss(training): 1.7557945477962493\n",
      "[epoch 31] loss(training): 1.7484021814912558\n",
      "[epoch 32] loss(training): 1.8265072368627244\n",
      "[epoch 33] loss(training): 1.7569211984222586\n",
      "[epoch 34] loss(training): 1.8508420330421491\n",
      "[epoch 35] loss(training): 1.7855140828138047\n",
      "[epoch 36] loss(training): 1.7646500824391842\n",
      "[epoch 37] loss(training): 1.8482995915683833\n",
      "[epoch 38] loss(training): 1.7508408389850096\n",
      "[epoch 39] loss(training): 1.7946171621571887\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(9, 7))\n",
    "ax[0].set_title(\"Clean\")\n",
    "ax[1].set_title(\"Noisy(input)\")\n",
    "ax[2].set_title(\"Denoised\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCH):\n",
    "\n",
    "    batch_train_loss = []\n",
    "\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        \n",
    "        # Noisy Images #\n",
    "        samples = [\n",
    "            data + (0.01 * torch.randn(len(data), 1, 120, 80)),\n",
    "            data + (0.02 * torch.randn(len(data), 1, 120, 80)),\n",
    "            data + (0.03 * torch.randn(len(data), 1, 120, 80)),\n",
    "            data + (0.04 * torch.randn(len(data), 1, 120, 80))\n",
    "        ]\n",
    "        ########\n",
    "\n",
    "        loss_train = train(batch_idx, epoch, data, samples[random.randint(0, 3)])\n",
    "\n",
    "        batch_train_loss.append(loss_train)\n",
    "\n",
    "    loss_train_mean.append(np.mean(batch_train_loss))\n",
    "    loss_train_std.append(np.std(batch_train_loss))\n",
    "\n",
    "    print(\"[epoch %s] loss(training): %s\" % (epoch, loss_train_mean[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLL9kyTBWmRf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-526265c86cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtestdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "testdata = np.load('test.npy')\n",
    "testdataset = numpyDataset(testdata, transform)\n",
    "testloader = DataLoader(testdataset, batch_size=1, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "result_for_submit = None  # this is for submit file\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "to_img = transforms.ToPILImage()\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(9, 7))\n",
    "for batch_idx, data in enumerate(testloader):\n",
    "    result_of_test = data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(7,9))\n",
    "        result_of_test = model.forward(data.cuda())\n",
    "        ax[0].imshow(to_img(data[0].cpu()), cmap='gray')\n",
    "        ax[1].imshow(to_img(y[0].cpu()), cmap='gray')\n",
    "        fig.show()\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        result_for_submit = result_of_test\n",
    "    else:\n",
    "        try:\n",
    "            result_for_submit = torch.cat([result_for_submit, result_of_test], dim=0)\n",
    "\n",
    "        except RuntimeError:\n",
    "            transposed = torch.transpose(result_of_test, 2, 3)\n",
    "            result_for_submit = torch.cat([result_for_submit, transposed], dim=0)\n",
    "\n",
    "# the submit_file.shape must be (400,1,120,80)\n",
    "submit_file = result_for_submit.detach().numpy()\n",
    "print(submit_file.shape)\n",
    "np.save('your_name.npy', submit_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 08-Copy1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
